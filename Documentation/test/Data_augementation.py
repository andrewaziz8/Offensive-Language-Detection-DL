# -*- coding: utf-8 -*-
"""MMAI894_DL_Project Part II_Data Augementation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HxSRLCeXDagmxLYYepVXEStcTeMnm4ES

##Import Libraries
"""

# !pip install datasets
!pip install transformers
# !pip install flair

import tensorflow.keras as keras
import pandas as pd
# from datasets import load_dataset
import numpy as np
from numpy import zeros
import tensorflow.keras as keras
from tensorflow.keras.layers import Dense, Input, Dropout, Embedding, Bidirectional, LSTM, GRU, Flatten, LayerNormalization, BatchNormalization
from tensorflow.keras.preprocessing.text import Tokenizer, one_hot
from keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import regularizers

from nltk.tokenize import TweetTokenizer
# from transformers import AutoTokenizer, AutoModel


# import flair
# from flair.embeddings import WordEmbeddings
# from flair.embeddings import ELMoEmbeddings
# from flair.embeddings import TransformerWordEmbeddings
# from flair.data import Sentence

from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, average_precision_score, precision_score, recall_score
from sklearn.utils.class_weight import compute_class_weight

from gensim.models.doc2vec import Doc2Vec, TaggedDocument

import re

# tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
# model = AutoModel.from_pretrained("bert-base-uncased")
# from pandas_profiling import ProfileReport


# dbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

# try: # this is only working on the 2nd try in colab :)
#   from transformers import DistilBertTokenizer, TFDistilBertModel
# except Exception as err: # so we catch the error and import it again
#   from transformers import DistilBertTokenizer, TFDistilBertModel

# print(help(TweetTokenizer))

"""##Data Preparation"""

#load data
def load_data():
  raw_data_df = pd.read_csv('https://query.data.world/s/twuhmzuhvitwqqcjh5picrq3qykr4r')
  return raw_data_df


###############################################################################################################
#clean data copied from https://www.kaggle.com/amackcrane/python-version-of-glove-twitter-preprocess-script
FLAGS = re.MULTILINE | re.DOTALL
def hashtag(text):
  text = text.group()
  hashtag_body = text[1:]
  if hashtag_body.isupper():
      result = "<hashtag> {} <allcaps>".format(hashtag_body.lower())
  else:
      result = " ".join(["<hashtag>"] + re.split(r"(?=[A-Z])", hashtag_body, flags=FLAGS))
  return result

def allcaps(text):
    text = text.group()
    return text.lower() + " <allcaps> " # amackcrane added trailing space
    # function so code less repetitive

def clean_data(text):
  eyes = r"[8:=;]"
  nose = r"['`\-]?"
  def re_sub(pattern, repl):
      return re.sub(pattern, repl, text, flags=FLAGS)

  text = re_sub(r"https?:\/\/\S+\b|www\.(\w+\.)+\S*", "<url>")
  text = re_sub(r"@\w+", "<user>")
  text = re_sub(r"{}{}[)dD]+|[)dD]+{}{}".format(eyes, nose, nose, eyes), "<smile>")
  text = re_sub(r"{}{}p+".format(eyes, nose), "<lolface>")
  text = re_sub(r"{}{}\(+|\)+{}{}".format(eyes, nose, nose, eyes), "<sadface>")
  text = re_sub(r"{}{}[\/|l*]".format(eyes, nose), "<neutralface>")
  text = re_sub(r"/"," / ")
  text = re_sub(r"<3","<heart>")
  text = re_sub(r"[-+]?[.\d]*[\d]+[:,.\d]*", "<number>")
  text = re_sub(r"#\w+", hashtag)  # amackcrane edit
  text = re_sub(r"([!?.]){2,}", r"\1 <repeat>")
  text = re_sub(r"\b(\S*?)(.)\2{2,}\b", r"\1\2 <elong>")
    

  ## -- I just don't understand why the Ruby script adds <allcaps> to everything so I limited the selection.
  # text = re_sub(r"([^a-z0-9()<>'`\-]){2,}", allcaps)
  #text = re_sub(r"([A-Z]){2,}", allcaps)  # moved below -amackcrane

  # amackcrane additions
  text = re_sub(r"([a-zA-Z<>()])([?!.:;,])", r"\1 \2")
  text = re_sub(r"\(([a-zA-Z<>]+)\)", r"( \1 )")
  text = re_sub(r"  ", r" ")
  text = re_sub(r" ([A-Z]){2,} ", allcaps)
    
  return text.lower()

def preprocessing_tweet(tweet):
  for t in tweet:
    t = clean_data(t)
  return tweet
###########################################################################################################



#split train val test
def split_data(cleantweet):
  trainX, tempX = train_test_split(cleantweet, test_size=0.4, random_state=42)
  valX, testX = train_test_split(tempX, test_size=0.5, random_state=42)
  return trainX, valX, testX

#extract tweet and y
def extract_tweet_and_y(raw_data_df):
  tweet, target = raw_data_df['tweet'], raw_data_df['class']
  return tweet, target

#tokenize and vectorize input using keras tokenizer
def keras_tokenizer(tweet_train, tweet_val, tweet_test, maxnumwords):
  # maxnumwords = 2000
  kt = Tokenizer()
  kt.fit_on_texts(tweet_train)
  word_index = kt.word_index
  vocab_size = len(word_index) + 1

  train_vectors = kt.texts_to_sequences(tweet_train) #Converting text to a vector of word indexes
  val_vectors = kt.texts_to_sequences(tweet_val) #Converting text to a vector of word indexes
  test_vectors = kt.texts_to_sequences(tweet_test) #Converting text to a vector of word indexes
  
  train_padded = pad_sequences(train_vectors, maxlen=maxnumwords, padding='post')
  val_padded = pad_sequences(val_vectors, maxlen=maxnumwords, padding='post')
  test_padded = pad_sequences(test_vectors, maxlen=maxnumwords, padding='post')

  return  train_padded, val_padded, test_padded, vocab_size, word_index

def hf_tokenizer(tweet):
  # AutoTokenizer
  pass

#GloVe embeddings using Glove twiter 100D
def GloveTwitterEmbedding(vocab_size, word_index):
  
  #Glove Twitter 100d
  embedding_path = "/content/drive/MyDrive/Colab Notebooks/GloVe Twitter 27B/glove.twitter.27B.100d.txt"
  # max_features = 30000
  # get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')
  # embedding_index = dict(get_coefs(*o.strip().split(" ")) for o in open(embedding_path))
  embedding_index = dict(
      (o.strip().split(" ")[0], np.array(o.strip().split(" ")[1:], dtype="float32")
      ) for o in open(embedding_path)
      )
  # embedding matrix
  embedding_matrix = zeros((vocab_size, 100))
  # for word, i in enumerate(tweet_tokenized):
  for t , i in enumerate(word_index.items()):
    embedding_vector = embedding_index.get(t)
    if embedding_vector is not None:
      embedding_matrix[i] = embedding_vector
  
  return embedding_matrix

def prepare_target(raw_y):
  class_weight = compute_class_weight('balanced', np.arange(3), raw_y)
  class_weight_dict = dict((c,w) for c, w in enumerate(class_weight))
  target = to_categorical(raw_y)
  return np.array(target), class_weight_dict


# #GloVe embeddings DO NOT USE
# def GloveEmbeddingmodel2(tweet):
#   glove_embedding = WordEmbeddings('glove')
#   tweet_glove_embedding=[]
#   for t in tweet:
#     temp_embedding=[]
#     sentence = Sentence(t)
#     glove_embedding.embed(sentence)
#     for token in sentence:
#       temp_embedding.append(token.embedding)
#     tweet_glove_embedding.append(temp_embedding)
#   return tweet_glove_embedding


# #tokenizers
# def tweet_tokenizer(tweet):
#   tt = TweetTokenizer(strip_handles=True)
#   tweet_tokenized = [tt.tokenize(l) for l in tweet]
#   return tweet_tokenized

#   tweet_tokenized = tweet_tokenizer(tweet)
#   tweet_tokenized_flatten = [item for sublist in tweet_tokenized for item in sublist]
#   tweet_tokenized_unique = np.unique(np.array(tweet_tokenized_flatten))
#   vocab_size = len(tweet_tokenized_unique)

#doc2vec embeddings using traing data only
# def build_d2vmodel(tweet_tokenized):
#   #prepare training data in doc2vec format:
#   train_doc2vec = [TaggedDocument((f), tags=[str(i)]) for i, f in enumerate(tweet_tokenized)]
#   #Train a doc2vec model to learn tweet representations. Use only training data!!
#   D2V_model = Doc2Vec(vector_size=50, alpha=0.025, min_count=5, dm =1, epochs=100)
#   D2V_model.build_vocab(train_doc2vec)
#   D2V_model.train(train_doc2vec, total_examples=model.corpus_count, epochs=model.epochs)
#   D2V_model.save("d2v.model")

# #vectorize tweet using Doc2Vec
# def encod_tweet_d2v(tweet_tokenized):
#   #Infer the feature representation for training and test data using the trained model
#   model= Doc2Vec.load("d2v.model")
#   #infer in multiple steps to get a stable representation. 
#   tweet_vectors =  [model.infer_vector(list_of_tokens, steps=50) for list_of_tokens in tweet_tokenized]

# #word 2 vec embeddings, 
# def build_w2vmodel(tweet_tokenized):
#   pass

# #one hot encode
# def one_hot_encode(text):
#   vocab_size = 100
#   max_length = 4
#   text_one_hot = [one_hot(t, vocab_size) for t in text]
#   return text_one_hot

"""# Data Augmentation"""

import random
import re
import pandas as pd
from nltk import sent_tokenize
from tqdm import tqdm
from albumentations.core.transforms_interface import DualTransform, BasicTransform
import nltk
nltk.download('punkt')

SENT_DETECTOR = nltk.data.load('tokenizers/punkt/english.pickle')

class NLPTransform(BasicTransform):
    """ Transform for nlp task."""
    LANGS = {
        'en': 'english',
        'it': 'italian', 
        'fr': 'french', 
        'es': 'spanish',
        'tr': 'turkish', 
        'ru': 'russian',
        'pt': 'portuguese'
    }

    @property
    def targets(self):
        return {"data": self.apply}
    
    def update_params(self, params, **kwargs):
        if hasattr(self, "interpolation"):
            params["interpolation"] = self.interpolation
        if hasattr(self, "fill_value"):
            params["fill_value"] = self.fill_value
        return params

    def get_sentences(self, text, lang='en'):
        return sent_tokenize(text, self.LANGS.get(lang, 'english'))

class ShuffleSentencesTransform(NLPTransform):
    """ Do shuffle by sentence """
    def __init__(self, always_apply=False, p=0.5):
        super(ShuffleSentencesTransform, self).__init__(always_apply, p)

    def apply(self, data, **params):
        text, lang = data
        sentences = self.get_sentences(text, lang)
        random.shuffle(sentences)
        return ' '.join(sentences), lang

transform = ShuffleSentencesTransform(p=2.0)

!pip install nlpaug

import nlpaug.augmenter.char as nac
import nlpaug.augmenter.word as naw
import nlpaug.augmenter.sentence as nas
import nlpaug.flow as naf

from nlpaug.util import Action
aug_insert_bert = naw.ContextualWordEmbsAug(
      model_path='bert-base-uncased', action="insert")

aug_substitute_bert =  naw.ContextualWordEmbsAug(
    model_path='roberta-base', action="substitute")

aug_substitute_bert.aug_p=0.2

aug_wordnet = naw.SynonymAug(aug_src='wordnet')

aug_swap = naw.RandomWordAug(action="swap")

aug_delete = naw.RandomWordAug()


def data_augment_bert_sw(aug_insert, aug_substitute, aug_swap, aug_delete, text):
  
  augmented_text = aug_insert.augment(text)
  augmented_text = aug_substitute.augment(augmented_text)
  augmented_text = aug_swap.augment(augmented_text)

  print("Original:")
  print(text)
  print("Augmented Text:")
  print(augmented_text)
  
  return augmented_text

def data_augment_wordnet_de(aug_insert, aug_wordnet,aug_delete, text):
  
  augmented_text = aug_insert.augment(text)
  augmented_text = aug_wordnet.augment(augmented_text)
  augmented_text = aug_delete.augment(augmented_text)
  print("Original:")
  print(text)
  print("Augmented Text:")
  print(augmented_text)
  
  return augmented_text


def Sentence_order_switch(text):
  lang = 'en'
  print("Original Text")
  print(text)
  print("Redorder Text")
  Redorder_txt = transform(data=(text, lang))['data'][0]

  return Redorder_txts

data_augment_bert(aug_insert_bert, aug_substitute_bert, "!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby4life: You ever fuck a bitch and she start to cry? You be confused as shit")

data_augment_wordnet_de(aug_insert_bert, aug_wordnet, aug_delete, "!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby4life: You ever fuck a bitch and she start to cry? You be confused as shit")

Sentence_order_switch("!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby4life: You ever fuck a bitch and she start to cry? You be confused as shit")



#Hate_df['tweet_reorder'] = Hate_df.apply(lambda row : data_augment_bert(aug_insert, aug_substitute, row['tweet']), axis =1)

"""##Build Albert's Model & Transfer Learning Model"""

raw_df = load_data()

clean_df = raw_df
clean_df['tweet'] = preprocessing_tweet(clean_df['tweet'])

train_df, val_df, test_df = split_data(clean_df)

X_train, y_train = extract_tweet_and_y(train_df)
X_val, y_val = extract_tweet_and_y(val_df)
X_test, y_test = extract_tweet_and_y(test_df)

y_train, class_weight_train = prepare_target(y_train)
y_val, class_weight_val = prepare_target(y_val)
y_test, class_weight_test = prepare_target(y_test)

Hate_df = clean_df[clean_df['class']==0]

A = Hate_df['tweet'].apply(lambda x: transform(data=(x, lang))['data'][0])

A[24685]

Hate_df['tweet'][24685]